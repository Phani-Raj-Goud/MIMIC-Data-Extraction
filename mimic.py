# -*- coding: utf-8 -*-
"""mimic.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KijV8jCux6-7NlCoQZBvygCoZ_6d3gIo

#MIMIC Dataset ICD Code extraction
"""

!pip install chromadb sentence-transformers

import kagglehub
import nltk
import pandas as pd
import chromadb
from chromadb.utils import embedding_functions

"""### Download Dataset containing patient discharge notes"""

path1 = kagglehub.dataset_download("mehrnooshazizi/mimic-iv-dataset")
print("Path to dataset files:", path1)

"""### Download Dataset containing MIMIC - III ICD 9 codes"""

path2 = kagglehub.dataset_download("bilal1907/mimic-iii-10k")
print("Path to dataset files:", path2)

"""### Load ICD codes for diagnoses"""

icd_dignoses_df = pd.read_csv(path2+'/MIMIC -III (10000 patients)/D_ICD_DIAGNOSES/D_ICD_DIAGNOSES.csv')

icd_dignoses_df.head()

EMBEDDING_MODEL = "all-MiniLM-L6-v2"
DB_PATH = "./chromadb_icd_codes"
COLLECTION_NAME = "icd_codes_collection"
TOP_K_RESULTS = 1
MAX_BATCH_SIZE = 5000 # Max batch size for chromadb is 5461

"""### Fucntion to chunck ICD codes into processable batch sizes"""

def chunk_list(data_list, batch_size):
    for i in range(0, len(data_list), batch_size):
        yield data_list[i:i + batch_size]

"""### Function to create chromaDB vector database consisting of ICD 9 code descriptions"""

def setup_chroma_db_and_load_data(df):
    print(f"Initializing ChromaDB with model: {EMBEDDING_MODEL}")

    # Initialize the embedding function
    hf_ef = embedding_functions.SentenceTransformerEmbeddingFunction(
        model_name=EMBEDDING_MODEL
    )

    # Initialize ChromaDB client (local storage)
    client = chromadb.PersistentClient(path=DB_PATH)

    # Create a collection
    collection = client.get_or_create_collection(
        name=COLLECTION_NAME,
        embedding_function=hf_ef
    )

    print(f"Loaded {len(df)} records from data source.")


    documents = df['LONG_TITLE'].tolist()
    metadatas = [{'icd9_code': code} for code in df['ICD9_CODE'].tolist()]
    ids = [f"doc_{i}" for i in range(len(df))]

    doc_chunks = list(chunk_list(documents, MAX_BATCH_SIZE))
    meta_chunks = list(chunk_list(metadatas, MAX_BATCH_SIZE))
    id_chunks = list(chunk_list(ids, MAX_BATCH_SIZE))

    total_chunks = len(doc_chunks)
    print(f"Splitting data into {total_chunks} batches of size up to {MAX_BATCH_SIZE}.")

    for i in range(total_chunks):
        print(f"  -> Processing batch {i + 1}/{total_chunks} (Size: {len(doc_chunks[i])})")

        # Add the current batch of documents, metadata, and IDs
        collection.add(
            documents=doc_chunks[i],
            metadatas=meta_chunks[i],
            ids=id_chunks[i]
        )

    print(f"\nSuccessfully added ALL data to ChromaDB collection: {COLLECTION_NAME}")
    return collection

"""### Function to perform rag"""

def find_relevant_codes(collection, excerpt):

    # The query method embeds the excerpt' and compares its vector against all vectors in the collection.
    results = collection.query(
        query_texts=[excerpt],
        n_results=TOP_K_RESULTS,
        include=['metadatas', 'distances', 'documents']
    )
    relevant_codes = []

    if results and 'metadatas' in results and results['metadatas']:
        for i in range(len(results['metadatas'][0])):
            metadata = results['metadatas'][0][i]
            distance = results['distances'][0][i]
            description = results['documents'][0][i]

            relevant_codes.append({
                'icd9_code': metadata['icd9_code'],
                'similarity_score': round(1 - distance, 4), # 1 - distance gives a score closer to 1 for high similarity
                'original_description': description
            })

    return relevant_codes

"""### Create ICD9 code vector database"""

icd9_codes_collection = setup_chroma_db_and_load_data(icd_dignoses_df)

"""### Fucntion to break text into sentences"""

def segment_text_into_sentences(text):
    # Ensure the required tokenizer data is downloaded
    try:
        nltk.data.find('tokenizers/punkt_tab')
    except LookupError:
        nltk.download('punkt_tab')

    # Use sent_tokenize to split the text
    sentences = nltk.sent_tokenize(text)

    return sentences

"""### Take the top 5 text description of patient discharge data for ICD code extraction"""

discharge_notes_df = pd.read_csv(path1+"/mimic_iv_summarization_test_dataset_shortened.csv")
excerpts = discharge_notes_df['text'].head(5).tolist()

"""### Extract IDC codes using"""

icd9_code_dict = {}
icd9_code_list = []
for excerpt in excerpts:
  icd9_code_set = set()
  for sentence in segment_text_into_sentences(excerpt):
    recommended_codes = find_relevant_codes(icd9_codes_collection, sentence)[0]
    if recommended_codes['similarity_score'] < 0.5:
      continue
    icd9_code_dict[recommended_codes['icd9_code']] = recommended_codes['original_description']
    icd9_code_set.add(recommended_codes['icd9_code'])
  icd9_code_list.append(list(icd9_code_set))

"""### Display extracted ICD codes for the original text"""

for i, excerpt in enumerate(excerpts):
  print('-------------------------------------------------------------------Original text---------------------------------------------------------')
  print(excerpt)
  print('---------------------------------------------------------------Extracted ICD codes-------------------------------------------------------')
  for icd9_code in icd9_code_list[i]:
    print(f"{icd9_code} : {icd9_code_dict[icd9_code]}" )

